{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import queue\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import wandb\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from cleanba import cleanba_impala\n",
    "from cleanba.environments import BoxobanConfig, EnvConfig\n",
    "from cleanba.network import Policy\n",
    "\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class EvalConfig:\n",
    "    env: EnvConfig\n",
    "    n_episode_multiple: int = 1\n",
    "    steps_to_think: list[int] = dataclasses.field(default_factory=lambda: [0])\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    safeguard_max_episode_steps: int = 30000\n",
    "\n",
    "    def run(self, policy: Policy, get_action_fn, params, *, key: jnp.ndarray) -> dict[str, float]:\n",
    "        key, env_key, carry_key, obs_reset_key = jax.random.split(key, 4)\n",
    "        env_seed = int(jax.random.randint(env_key, (), minval=0, maxval=2**31 - 2))\n",
    "        envs = dataclasses.replace(self.env, seed=env_seed).make()\n",
    "        max_steps = min(self.safeguard_max_episode_steps, self.env.max_episode_steps)\n",
    "\n",
    "        episode_starts_no = jnp.zeros(envs.num_envs, dtype=jnp.bool_)\n",
    "\n",
    "        metrics = {}\n",
    "        try:\n",
    "            for steps_to_think in self.steps_to_think:\n",
    "                all_episode_returns = []\n",
    "                all_episode_lengths = []\n",
    "                all_episode_successes = []\n",
    "                all_obs = []\n",
    "                all_acts = []\n",
    "                all_rewards = []\n",
    "                all_level_infos = []\n",
    "                envs = dataclasses.replace(self.env, seed=env_seed).make()\n",
    "                reset_key = None\n",
    "                for _ in range(self.n_episode_multiple):\n",
    "                    reset_key, sub_reset_key = jax.random.split(obs_reset_key if reset_key is None else reset_key)\n",
    "                    reset_seed = int(jax.random.randint(sub_reset_key, (), minval=0, maxval=2**31 - 2))\n",
    "                    obs, level_infos = envs.reset(seed=reset_seed)\n",
    "                    # reset the carry here so we can use `episode_starts_no` later\n",
    "                    carry = policy.apply(params, carry_key, obs.shape, method=policy.initialize_carry)\n",
    "\n",
    "                    # Update the carry with the initial observation many times\n",
    "                    for think_step in range(steps_to_think):\n",
    "                        carry, _, _, key = get_action_fn(\n",
    "                            params, carry, obs, episode_starts_no, key, temperature=self.temperature\n",
    "                        )\n",
    "\n",
    "                    eps_done = np.zeros(envs.num_envs, dtype=np.bool_)\n",
    "                    episode_success = np.zeros(envs.num_envs, dtype=np.bool_)\n",
    "                    episode_returns = np.zeros(envs.num_envs, dtype=np.float64)\n",
    "                    episode_lengths = np.zeros(envs.num_envs, dtype=np.int64)\n",
    "                    episode_obs = np.zeros((max_steps+1, *obs.shape), dtype=np.int64)\n",
    "                    episode_acts = np.zeros((max_steps, envs.num_envs), dtype=np.int64)\n",
    "                    episode_rewards = np.zeros((max_steps, envs.num_envs), dtype=np.float64)\n",
    "                    \n",
    "                    episode_obs[0] = obs\n",
    "                    i = 0\n",
    "                    while not np.all(eps_done):\n",
    "                        if i >= self.safeguard_max_episode_steps:\n",
    "                            break\n",
    "                        carry, action, _, key = get_action_fn(\n",
    "                            params, carry, obs, episode_starts_no, key, temperature=self.temperature\n",
    "                        )\n",
    "\n",
    "                        cpu_action = np.asarray(action)\n",
    "                        obs, rewards, terminated, truncated, infos = envs.step(cpu_action)\n",
    "                        episode_returns[~eps_done] += rewards[~eps_done]\n",
    "                        episode_lengths[~eps_done] += 1\n",
    "                        episode_success[~eps_done] |= terminated[~eps_done]  # If episode terminates it's a success\n",
    "\n",
    "                        episode_obs[i+1, ~eps_done] = obs[~eps_done]\n",
    "                        episode_acts[i, ~eps_done] = cpu_action[~eps_done]\n",
    "                        episode_rewards[i, ~eps_done] = rewards[~eps_done]\n",
    "\n",
    "                        # Set as done the episodes which are done\n",
    "                        eps_done |= truncated | terminated\n",
    "                        i += 1\n",
    "\n",
    "                    all_episode_returns.append(episode_returns)\n",
    "                    all_episode_lengths.append(episode_lengths)\n",
    "                    all_episode_successes.append(episode_success)\n",
    "\n",
    "                    all_obs += [episode_obs[:episode_lengths[i], i] for i in range(envs.num_envs)]\n",
    "                    all_acts += [episode_acts[:episode_lengths[i], i] for i in range(envs.num_envs)]\n",
    "                    all_rewards += [episode_rewards[:episode_lengths[i], i] for i in range(envs.num_envs)]\n",
    "\n",
    "                    all_obs.append(episode_obs)\n",
    "                    all_acts.append(episode_acts)\n",
    "                    all_level_infos.append(level_infos)\n",
    "\n",
    "                all_episode_returns = np.concatenate(all_episode_returns)\n",
    "                all_episode_lengths = np.concatenate(all_episode_lengths)\n",
    "                all_episode_successes = np.concatenate(all_episode_successes)\n",
    "                all_level_infos = {k: np.concatenate([d[k] for d in all_level_infos])\n",
    "                                    for k in all_level_infos[0].keys() if not k.startswith(\"_\")}\n",
    "\n",
    "                metrics.update(\n",
    "                    {\n",
    "                        f\"{steps_to_think:02d}_episode_returns\": float(np.mean(all_episode_returns)),\n",
    "                        f\"{steps_to_think:02d}_episode_lengths\": float(np.mean(all_episode_lengths)),\n",
    "                        f\"{steps_to_think:02d}_episode_successes\": float(np.mean(all_episode_successes)),\n",
    "                        f\"{steps_to_think:02d}_num_episodes\": len(all_episode_returns),\n",
    "                        f\"{steps_to_think:02d}_all_episode_info\": dict(\n",
    "                            episode_returns=all_episode_returns,\n",
    "                            episode_lengths=all_episode_lengths,\n",
    "                            episode_successes=all_episode_successes,\n",
    "                            episode_obs=all_obs,\n",
    "                            episode_acts=all_acts,\n",
    "                            episode_rewards=all_rewards,\n",
    "                            level_infos=level_infos,\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "        finally:\n",
    "            envs.close()\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_to_think=[0, 1, 2, 3, 4, 6, 8, 10]\n",
    "n_episode_multiple = 20\n",
    "num_envs = 100\n",
    "unfil_env_cfg = EvalConfig(\n",
    "    BoxobanConfig(\n",
    "        split=\"test\",\n",
    "        difficulty=\"unfiltered\",\n",
    "        min_episode_steps=240,\n",
    "        max_episode_steps=240,\n",
    "        num_envs=num_envs,\n",
    "        tinyworld_obs=True,\n",
    "        seed=42,\n",
    "    ),\n",
    "    n_episode_multiple=n_episode_multiple,\n",
    "    steps_to_think=steps_to_think,\n",
    "\n",
    ")\n",
    "\n",
    "val_med_env_cfg = EvalConfig(\n",
    "    BoxobanConfig(\n",
    "        split=\"valid\",\n",
    "        difficulty=\"medium\",\n",
    "        min_episode_steps=240,\n",
    "        max_episode_steps=240,\n",
    "        num_envs=num_envs,\n",
    "        tinyworld_obs=True,\n",
    "        seed=42,\n",
    "    ),\n",
    "    n_episode_multiple=n_episode_multiple,\n",
    "    steps_to_think=steps_to_think,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_med_env_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = pathlib.Path(\"/training/cleanba/logs/data\")\n",
    "if not (base_path / \"unfil_log_dict.pkl\").exists():\n",
    "    path = pathlib.Path(\"/training/cleanba/044-more-planners/wandb/run-20240506_043059-6zhw6cw1/local-files/cp_208000000\")\n",
    "    args, train_state = cleanba_impala.load_train_state(path)\n",
    "    prng_key = jax.random.PRNGKey(0)\n",
    "    policy, carry_t, _ = args.net.init_params(args.train_env.make(), prng_key)\n",
    "    get_action_fn = jax.jit(partial(policy.apply, method=policy.get_action), static_argnames=\"temperature\")\n",
    "    params = train_state.params\n",
    "\n",
    "    unfil_log_dict = unfil_env_cfg.run(policy, get_action_fn, params, key=prng_key)\n",
    "    unfil_all_episode_info = [unfil_log_dict.pop(f\"{steps_to_think:02d}_all_episode_info\") for steps_to_think in steps_to_think]\n",
    "\n",
    "    print(\"finished unfiltered\")\n",
    "\n",
    "    val_log_dict = val_med_env_cfg.run(policy, get_action_fn, params, key=prng_key)\n",
    "    val_all_episode_info = [val_log_dict.pop(f\"{steps_to_think:02d}_all_episode_info\") for steps_to_think in steps_to_think]\n",
    "\n",
    "    import pickle\n",
    "    with open(base_path / \"unfil_log_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(unfil_log_dict, f)\n",
    "    with open(base_path / \"unfil_all_episode_info.pkl\", \"wb\") as f:\n",
    "        pickle.dump(unfil_all_episode_info, f)\n",
    "    with open(base_path / \"val_log_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(val_log_dict, f)\n",
    "    with open(base_path / \"val_all_episode_info.pkl\", \"wb\") as f:\n",
    "        pickle.dump(val_all_episode_info, f)\n",
    "\n",
    "else:\n",
    "    print(\"loading logs\")\n",
    "    import pickle\n",
    "    with open(base_path / \"unfil_log_dict.pkl\", \"rb\") as f:\n",
    "        unfil_log_dict = pickle.load(f)\n",
    "    with open(base_path / \"unfil_all_episode_info.pkl\", \"rb\") as f:\n",
    "        unfil_all_episode_info = pickle.load(f)\n",
    "    with open(base_path / \"val_log_dict.pkl\", \"rb\") as f:\n",
    "        val_log_dict = pickle.load(f)\n",
    "    with open(base_path / \"val_all_episode_info.pkl\", \"rb\") as f:\n",
    "        val_all_episode_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_names = [\"unfiltered_test\", \"valid_medium\"]\n",
    "for i, log_dict in enumerate([unfil_log_dict, val_log_dict]):\n",
    "    # plot XX_episode_successes across all XX which are steps_to_think\n",
    "    episode_successes = [log_dict[f\"{steps_to_think:02d}_episode_successes\"] for steps_to_think in steps_to_think]\n",
    "    episode_returns = [log_dict[f\"{steps_to_think:02d}_episode_returns\"] for steps_to_think in steps_to_think]\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('steps_to_think')\n",
    "    ax1.set_ylabel('episode_successes', color=color)\n",
    "    ax1.plot(steps_to_think, episode_successes, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('episode_returns', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(steps_to_think, episode_returns, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(env_names[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_levels = len(val_all_episode_info[0][\"episode_successes\"])\n",
    "improved_level_list = []\n",
    "impaired_level_list = []\n",
    "solved_better_returns = []\n",
    "solved_worse_returns = []\n",
    "unsolved_better_returns = []\n",
    "unsolved_worse_returns = []\n",
    "same_return_and_solve = []\n",
    "baseline_steps = 0\n",
    "best_steps = steps_to_think.index(6)\n",
    "for i in range(len(val_all_episode_info[0][\"episode_successes\"])):\n",
    "    solved_after_thinking = val_all_episode_info[baseline_steps][\"episode_successes\"][i] < \\\n",
    "        val_all_episode_info[best_steps][\"episode_successes\"][i]\n",
    "    messed_up_after_thinking = val_all_episode_info[baseline_steps][\"episode_successes\"][i] > \\\n",
    "        val_all_episode_info[best_steps][\"episode_successes\"][i]\n",
    "\n",
    "    solved_always = val_all_episode_info[baseline_steps][\"episode_successes\"][i] and \\\n",
    "        val_all_episode_info[best_steps][\"episode_successes\"][i]\n",
    "    unsolved_always = not(val_all_episode_info[baseline_steps][\"episode_successes\"][i] or \\\n",
    "        val_all_episode_info[best_steps][\"episode_successes\"][i])\n",
    "    better_return = val_all_episode_info[best_steps][\"episode_returns\"][i] > \\\n",
    "        val_all_episode_info[baseline_steps][\"episode_returns\"][i]\n",
    "    worse_return = val_all_episode_info[best_steps][\"episode_returns\"][i] < \\\n",
    "        val_all_episode_info[baseline_steps][\"episode_returns\"][i]\n",
    "\n",
    "    if solved_after_thinking:\n",
    "        improved_level_list.append(i)\n",
    "    elif messed_up_after_thinking:\n",
    "        impaired_level_list.append(i)\n",
    "    elif solved_always and better_return:\n",
    "        solved_better_returns.append(i)\n",
    "    elif solved_always and worse_return:\n",
    "        solved_worse_returns.append(i)\n",
    "    elif unsolved_always and better_return:\n",
    "        unsolved_better_returns.append(i)\n",
    "    elif unsolved_always and worse_return:\n",
    "        unsolved_worse_returns.append(i)\n",
    "    else:\n",
    "        same_return_and_solve.append(i)\n",
    "\n",
    "# print all fractions\n",
    "improved_pc = len(improved_level_list)/num_levels*100\n",
    "impaired_pc = len(impaired_level_list)/num_levels*100\n",
    "solved_better_pc = len(solved_better_returns)/num_levels*100\n",
    "solved_worse_pc = len(solved_worse_returns)/num_levels*100\n",
    "unsolved_better_pc = len(unsolved_better_returns)/num_levels*100\n",
    "unsolved_worse_pc = len(unsolved_worse_returns)/num_levels*100\n",
    "same_return_and_solve_pc = len(same_return_and_solve)/num_levels*100\n",
    "\n",
    "print(f\"Solved previously unsolved:\\t{improved_pc:.2f}%\")\n",
    "print(f\"Unsolved previously solved:\\t{impaired_pc:.2f}%\")\n",
    "print(f\"Solved w/ better returns:\\t{solved_better_pc:.2f}%\")\n",
    "print(f\"Solved but worse returns:\\t{solved_worse_pc:.2f}%\")\n",
    "print(f\"Unsolved but better returns:\\t{unsolved_better_pc:.2f}%\")\n",
    "print(f\"Unsolved w/ worse returns:\\t{unsolved_worse_pc:.2f}%\")\n",
    "print(f\"Same return & solve:\\t\\t{same_return_and_solve_pc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_level_video(level_idx, base_dir=\"./\"):\n",
    "    obs_baseline = np.moveaxis(val_all_episode_info[baseline_steps][\"episode_obs\"][level_idx], 1, 3)\n",
    "    obs_best = np.moveaxis(val_all_episode_info[best_steps][\"episode_obs\"][level_idx], 1, 3)\n",
    "    num_obs_baseline = len(obs_baseline)\n",
    "    num_obs_best = len(obs_best)\n",
    "    max_obs = max(num_obs_baseline, num_obs_best)\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.suptitle(f\"Level {i}\")\n",
    "    ax1, ax2 = axs\n",
    "    ax1.set_title(f\"{steps_to_think[baseline_steps]} think steps\")\n",
    "    ax2.set_title(f\"{steps_to_think[best_steps]} think steps\")\n",
    "    im1 = ax1.imshow(obs_baseline[0])\n",
    "    im2 = ax2.imshow(obs_best[0])\n",
    "    title = fig.suptitle(f\"Level {i}: Step 0\")\n",
    "\n",
    "    def update_frame(j):\n",
    "        baseline_img = obs_baseline[min(len(obs_baseline)-2, j)]\n",
    "        # ax1.imshow(baseline_img)\n",
    "        im1.set(data=baseline_img)\n",
    "        best_img = obs_best[min(len(obs_best)-2, j)]\n",
    "        # ax2.imshow(best_img)\n",
    "        im2.set(data=best_img)\n",
    "        title.set_text(f\"Level {i}: Step {j}\")\n",
    "        return (im1, im2, title)\n",
    "        \n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig,\n",
    "        update_frame,  # type: ignore\n",
    "        frames=max_obs,\n",
    "        interval=1,\n",
    "        repeat=False,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    base_dir = pathlib.Path(base_dir)\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    anim.save(base_dir / f'{i}.mp4', fps=3)\n",
    "    print(f\"Level {i} saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to place box vs thinking steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_for_placing_box = 0.9\n",
    "reward_for_placing_last_box = -0.1 + 1.0 + 10.0\n",
    "\n",
    "time_across_think_steps = []\n",
    "condition_on_improved_levels = True\n",
    "for j in range(len(steps_to_think)):\n",
    "    all_rewards = val_all_episode_info[j][\"episode_rewards\"]\n",
    "    if condition_on_improved_levels:\n",
    "        time_for_placing_boxes = [np.where(all_rewards[level_idx] == reward_for_placing_box)[0] for level_idx in improved_level_list]\n",
    "    else:\n",
    "        time_for_placing_boxes = [np.where(reward_array == reward_for_placing_box)[0] for reward_array in all_rewards]\n",
    "    avg_time_box_placed = [np.mean([t[box_idx] for t in time_for_placing_boxes if len(t) > box_idx]) for box_idx in range(3)]\n",
    "    time_for_placing_last_box = [np.where(reward_array == reward_for_placing_last_box)[0] for reward_array in all_rewards]\n",
    "    time_for_placing_last_box = [e for e in time_for_placing_last_box if len(e) > 0]\n",
    "    avg_time_box_placed.append(np.mean(time_for_placing_last_box))\n",
    "    time_across_think_steps.append(avg_time_box_placed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps_to_think, time_across_think_steps)\n",
    "plt.legend([\"Box 1\", \"Box 2\", \"Box 3\", \"Box 4\"])\n",
    "plt.xlabel(\"Steps to think\")\n",
    "plt.ylabel(\"Avg timesteps to place the box\")\n",
    "if condition_on_improved_levels:\n",
    "    plt.title(\"On levels where thinking more solves an unsolved level\")\n",
    "else:\n",
    "    plt.title(\"On all levels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps_to_think, time_across_think_steps)\n",
    "plt.legend([\"Box 1\", \"Box 2\", \"Box 3\", \"Box 4\"])\n",
    "plt.xlabel(\"Steps to think\")\n",
    "plt.ylabel(\"Avg timesteps to place the box\")\n",
    "plt.title(\"On levels where thinking more solves an unsolved level\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Already solved but better return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(steps_to_think)):\n",
    "    val_all_episode_info[i][\"episode_obs\"] = val_all_episode_info[i][\"episode_obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level_idx in solved_better_returns:\n",
    "    save_level_video(level_idx, base_dir=\"solved_better_returns/\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = cleanba_impala.WandbWriter(args)\n",
    "# param_queue = queue.Queue(maxsize=1)\n",
    "# rollout_queue = queue.Queue(maxsize=1)\n",
    "# learner_policy_version = 0\n",
    "# unreplicated_params = train_state.params\n",
    "# with cleanba_impala.initialize_multi_device(args) as runtime_info:\n",
    "#     device_params = jax.device_put(unreplicated_params, runtime_info.local_devices[0])\n",
    "#     param_queue.put((device_params, learner_policy_version))\n",
    "#     prng_key = jax.random.PRNGKey(0)\n",
    "#     cleanba_impala.rollout(\n",
    "#         prng_key,\n",
    "#         args,\n",
    "#         runtime_info,\n",
    "#         rollout_queue,\n",
    "#         param_queue,\n",
    "#         writer,\n",
    "#         runtime_info.learner_devices,\n",
    "#         0,\n",
    "#         runtime_info.local_devices[0],\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
