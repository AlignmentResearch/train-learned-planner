{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import pathlib\n",
    "import queue\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import wandb\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from cleanba import cleanba_impala\n",
    "from cleanba.environments import BoxobanConfig, EnvConfig\n",
    "from cleanba.network import Policy\n",
    "\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class EvalConfig:\n",
    "    env: EnvConfig\n",
    "    n_episode_multiple: int = 1\n",
    "    steps_to_think: list[int] = dataclasses.field(default_factory=lambda: [0])\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    safeguard_max_episode_steps: int = 30000\n",
    "\n",
    "    def run(self, policy: Policy, get_action_fn, params, *, key: jnp.ndarray) -> dict[str, float]:\n",
    "        key, env_key, carry_key, obs_reset_key = jax.random.split(key, 4)\n",
    "        env_seed = int(jax.random.randint(env_key, (), minval=0, maxval=2**31 - 2))\n",
    "        envs = dataclasses.replace(self.env, seed=env_seed).make()\n",
    "\n",
    "        episode_starts_no = jnp.zeros(envs.num_envs, dtype=jnp.bool_)\n",
    "\n",
    "        metrics = {}\n",
    "        try:\n",
    "            for steps_to_think in self.steps_to_think:\n",
    "                all_episode_returns = []\n",
    "                all_episode_lengths = []\n",
    "                all_episode_successes = []\n",
    "                all_obs = []\n",
    "                all_acts = []\n",
    "                all_level_infos = []\n",
    "                envs = dataclasses.replace(self.env, seed=env_seed).make()\n",
    "                reset_key = None\n",
    "                for _ in range(self.n_episode_multiple):\n",
    "                    reset_key, sub_reset_key = jax.random.split(obs_reset_key if reset_key is None else reset_key)\n",
    "                    reset_seed = int(jax.random.randint(sub_reset_key, (), minval=0, maxval=2**31 - 2))\n",
    "                    obs, level_infos = envs.reset(seed=reset_seed)\n",
    "                    # reset the carry here so we can use `episode_starts_no` later\n",
    "                    carry = policy.apply(params, carry_key, obs.shape, method=policy.initialize_carry)\n",
    "\n",
    "                    # Update the carry with the initial observation many times\n",
    "                    for think_step in range(steps_to_think):\n",
    "                        carry, _, _, key = get_action_fn(\n",
    "                            params, carry, obs, episode_starts_no, key, temperature=self.temperature\n",
    "                        )\n",
    "\n",
    "                    eps_done = np.zeros(envs.num_envs, dtype=np.bool_)\n",
    "                    episode_success = np.zeros(envs.num_envs, dtype=np.bool_)\n",
    "                    episode_returns = np.zeros(envs.num_envs, dtype=np.float64)\n",
    "                    episode_lengths = np.zeros(envs.num_envs, dtype=np.int64)\n",
    "                    episode_obs = [obs]\n",
    "                    episode_acts = []\n",
    "\n",
    "                    i = 0\n",
    "                    while not np.all(eps_done):\n",
    "                        i += 1\n",
    "                        if i >= self.safeguard_max_episode_steps:\n",
    "                            break\n",
    "                        carry, action, _, key = get_action_fn(\n",
    "                            params, carry, obs, episode_starts_no, key, temperature=self.temperature\n",
    "                        )\n",
    "\n",
    "                        cpu_action = np.asarray(action)\n",
    "                        obs, rewards, terminated, truncated, infos = envs.step(cpu_action)\n",
    "                        episode_returns[~eps_done] += rewards[~eps_done]\n",
    "                        episode_lengths[~eps_done] += 1\n",
    "                        episode_success[~eps_done] |= terminated[~eps_done]  # If episode terminates it's a success\n",
    "\n",
    "                        # Set as done the episodes which are done\n",
    "                        eps_done |= truncated | terminated\n",
    "                        episode_obs.append(obs)\n",
    "                        episode_acts.append(cpu_action)\n",
    "\n",
    "\n",
    "                    all_episode_returns.append(episode_returns)\n",
    "                    all_episode_lengths.append(episode_lengths)\n",
    "                    all_episode_successes.append(episode_success)\n",
    "\n",
    "                    for i in range(envs.num_envs):\n",
    "                        all_obs.append(np.stack([episode_obs[j][i] for j in range(episode_lengths[i] + 1)]))\n",
    "                        all_acts.append(np.stack([episode_acts[j][i] for j in range(episode_lengths[i])]))\n",
    "\n",
    "                    all_obs.append(episode_obs)\n",
    "                    all_acts.append(episode_acts)\n",
    "                    all_level_infos.append(level_infos)\n",
    "\n",
    "                all_episode_returns = np.concatenate(all_episode_returns)\n",
    "                all_episode_lengths = np.concatenate(all_episode_lengths)\n",
    "                all_episode_successes = np.concatenate(all_episode_successes)\n",
    "                all_level_infos = {k: np.concatenate([d[k] for d in all_level_infos])\n",
    "                                    for k in all_level_infos[0].keys() if not k.startswith(\"_\")}\n",
    "\n",
    "                metrics.update(\n",
    "                    {\n",
    "                        f\"{steps_to_think:02d}_episode_returns\": float(np.mean(all_episode_returns)),\n",
    "                        f\"{steps_to_think:02d}_episode_lengths\": float(np.mean(all_episode_lengths)),\n",
    "                        f\"{steps_to_think:02d}_episode_successes\": float(np.mean(all_episode_successes)),\n",
    "                        f\"{steps_to_think:02d}_num_episodes\": len(all_episode_returns),\n",
    "                        f\"{steps_to_think:02d}_all_episode_info\": dict(\n",
    "                            episode_returns=all_episode_returns,\n",
    "                            episode_lengths=all_episode_lengths,\n",
    "                            episode_successes=all_episode_successes,\n",
    "                            episode_obs=all_obs,\n",
    "                            episode_acts=all_acts,\n",
    "                            level_infos=level_infos,\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "        finally:\n",
    "            envs.close()\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_to_think=[0, 1, 2, 3, 4, 6, 8, 10]\n",
    "n_episode_multiple = 20\n",
    "num_envs = 100\n",
    "unfil_env_cfg = EvalConfig(\n",
    "    BoxobanConfig(\n",
    "        split=\"test\",\n",
    "        difficulty=\"unfiltered\",\n",
    "        min_episode_steps=240,\n",
    "        max_episode_steps=240,\n",
    "        num_envs=num_envs,\n",
    "        tinyworld_obs=True,\n",
    "        seed=42,\n",
    "    ),\n",
    "    n_episode_multiple=n_episode_multiple,\n",
    "    steps_to_think=steps_to_think,\n",
    "\n",
    ")\n",
    "\n",
    "val_med_env_cfg = EvalConfig(\n",
    "    BoxobanConfig(\n",
    "        split=\"valid\",\n",
    "        difficulty=\"medium\",\n",
    "        min_episode_steps=240,\n",
    "        max_episode_steps=240,\n",
    "        num_envs=num_envs,\n",
    "        tinyworld_obs=True,\n",
    "        seed=42,\n",
    "    ),\n",
    "    n_episode_multiple=n_episode_multiple,\n",
    "    steps_to_think=steps_to_think,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_to_think=[0, 1, 2, 3, 4, 6, 8, 10]\n",
    "path = pathlib.Path(\"/training/cleanba/cp_208000000\")\n",
    "args, train_state = cleanba_impala.load_train_state(path)\n",
    "\n",
    "# unfil_env_cfg = EvalConfig(**args.eval_envs[\"test_unfiltered\"].__dict__)\n",
    "# val_med_env_cfg = EvalConfig(**args.eval_envs[\"valid_medium\"].__dict__)\n",
    "\n",
    "# unfil_env_cfg = dataclasses.replace(unfil_env_cfg, steps_to_think=steps_to_think, n_episode_multiple=5)\n",
    "# val_med_env_cfg = dataclasses.replace(val_med_env_cfg, steps_to_think=steps_to_think, n_episode_multiple=5)\n",
    "\n",
    "# unfil_env_cfg.env = dataclasses.replace(unfil_env_cfg.env, num_envs=200, load_sequentially=False)\n",
    "# val_med_env_cfg.env = dataclasses.replace(val_med_env_cfg.env, num_envs=200, load_sequentially=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_med_env_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng_key = jax.random.PRNGKey(0)\n",
    "policy, carry_t, _ = args.net.init_params(args.train_env.make(), prng_key)\n",
    "get_action_fn = jax.jit(partial(policy.apply, method=policy.get_action), static_argnames=\"temperature\")\n",
    "params = train_state.params\n",
    "unfil_log_dict = unfil_env_cfg.run(policy, get_action_fn, params, key=prng_key)\n",
    "unfil_all_episode_info = [unfil_log_dict.pop(f\"{steps_to_think:02d}_all_episode_info\") for steps_to_think in steps_to_think]\n",
    "\n",
    "print(\"finished unfiltered\")\n",
    "\n",
    "val_log_dict = val_med_env_cfg.run(policy, get_action_fn, params, key=prng_key)\n",
    "val_all_episode_info = [val_log_dict.pop(f\"{steps_to_think:02d}_all_episode_info\") for steps_to_think in steps_to_think]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_names = [\"unfiltered_test\", \"valid_medium\"]\n",
    "for i, log_dict in enumerate([unfil_log_dict, val_log_dict]):\n",
    "    # plot XX_episode_successes across all XX which are steps_to_think\n",
    "    episode_successes = [log_dict[f\"{steps_to_think:02d}_episode_successes\"] for steps_to_think in steps_to_think]\n",
    "    episode_returns = [log_dict[f\"{steps_to_think:02d}_episode_returns\"] for steps_to_think in steps_to_think]\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('steps_to_think')\n",
    "    ax1.set_ylabel('episode_successes', color=color)\n",
    "    ax1.plot(steps_to_think, episode_successes, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('episode_returns', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(steps_to_think, episode_returns, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(env_names[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "obs = np.moveaxis(val_all_episode_info[0][\"episode_obs\"][0], 1, 3)\n",
    "acts = val_all_episode_info[0][\"episode_acts\"][0]\n",
    "fig = plt.figure()  # Create a figure for the animation\n",
    "\n",
    "def update_frame(i):\n",
    "    if i > len(acts):\n",
    "        return\n",
    "    # if i != 0:\n",
    "    #     act = acts[i]\n",
    "    #     e.step(act)\n",
    "    # img = e.render()\n",
    "    img = obs[i]\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Step {i}\")  # Add index to title\n",
    "\n",
    "anim = animation.FuncAnimation(\n",
    "    fig,\n",
    "    update_frame,  # type: ignore\n",
    "    frames=len(acts) + 1,\n",
    "    interval=1,\n",
    "    repeat=False,\n",
    ")\n",
    "assert anim is not None\n",
    "# plt.show()\n",
    "writervideo = animation.FFMpegWriter(fps=3) \n",
    "anim.save('video.mp4', writer=writervideo) \n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_level_list = []\n",
    "baseline_steps = 0\n",
    "best_steps = steps_to_think.index(6)\n",
    "for i in range(len(val_all_episode_info[0][\"episode_successes\"])):\n",
    "    if val_all_episode_info[baseline_steps][\"episode_successes\"][i] < \\\n",
    "        val_all_episode_info[best_steps][\"episode_successes\"][i]:\n",
    "        improved_level_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in improved_level_list[2:]:\n",
    "    obs_baseline = np.moveaxis(val_all_episode_info[baseline_steps][\"episode_obs\"][i], 1, 3)\n",
    "    obs_best = np.moveaxis(val_all_episode_info[best_steps][\"episode_obs\"][i], 1, 3)\n",
    "    num_obs_baseline = len(obs_baseline)\n",
    "    num_obs_best = len(obs_best)\n",
    "    max_obs = max(num_obs_baseline, num_obs_best)\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.suptitle(f\"Level {i}\")\n",
    "    ax1, ax2 = axs\n",
    "    ax1.set_title(f\"{steps_to_think[baseline_steps]} think steps\")\n",
    "    ax2.set_title(f\"{steps_to_think[best_steps]} think steps\")\n",
    "    im1 = ax1.imshow(obs_baseline[0])\n",
    "    im2 = ax2.imshow(obs_best[0])\n",
    "    title = fig.suptitle(f\"Level {i}: Step 0\")\n",
    "\n",
    "    def update_frame(j):\n",
    "        baseline_img = obs_baseline[min(len(obs_baseline)-2, j)]\n",
    "        # ax1.imshow(baseline_img)\n",
    "        im1.set(data=baseline_img)\n",
    "        best_img = obs_best[min(len(obs_best)-2, j)]\n",
    "        # ax2.imshow(best_img)\n",
    "        im2.set(data=best_img)\n",
    "        title.set_text(f\"Level {i}: Step {j}\")\n",
    "        return (im1, im2, title)\n",
    "        \n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig,\n",
    "        update_frame,  # type: ignore\n",
    "        frames=max_obs,\n",
    "        interval=1,\n",
    "        repeat=False,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    # writervideo = animation.FFMpegWriter(fps=3) \n",
    "    # anim.save(f'improved_levels/{i}.mp4', writer=writervideo)\n",
    "    anim.save(f'improved_levels/{i}.mp4', fps=3)\n",
    "\n",
    "    # plt.close()\n",
    "    print(f\"Level {i} saved\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unfil_log_dict and unfil_all_episode_info to disk\n",
    "import pickle\n",
    "with open(\"logs/unfil_log_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(unfil_log_dict, f)\n",
    "with open(\"logs/unfil_all_episode_info.pkl\", \"wb\") as f:\n",
    "    pickle.dump(unfil_all_episode_info, f)\n",
    "with open(\"logs/val_log_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_log_dict, f)\n",
    "with open(\"logs/val_all_episode_info.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_all_episode_info, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = cleanba_impala.WandbWriter(args)\n",
    "# param_queue = queue.Queue(maxsize=1)\n",
    "# rollout_queue = queue.Queue(maxsize=1)\n",
    "# learner_policy_version = 0\n",
    "# unreplicated_params = train_state.params\n",
    "# with cleanba_impala.initialize_multi_device(args) as runtime_info:\n",
    "#     device_params = jax.device_put(unreplicated_params, runtime_info.local_devices[0])\n",
    "#     param_queue.put((device_params, learner_policy_version))\n",
    "#     prng_key = jax.random.PRNGKey(0)\n",
    "#     cleanba_impala.rollout(\n",
    "#         prng_key,\n",
    "#         args,\n",
    "#         runtime_info,\n",
    "#         rollout_queue,\n",
    "#         param_queue,\n",
    "#         writer,\n",
    "#         runtime_info.learner_devices,\n",
    "#         0,\n",
    "#         runtime_info.local_devices[0],\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
